{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e90ad76-4fdd-40d4-b616-d054b27bc85b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d755c0fb-6b17-4aea-a1ef-214bc56a657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ce1e669-c562-4e3d-a001-486f57fce386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2026.1.4: Fast Llama patching. Transformers: 4.57.1. vLLM: 0.11.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 5080 Laptop GPU. Num GPUs = 1. Max memory: 15.92 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Loaded\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n",
    "    max_seq_length = 2048,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "print(\"Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce4e48bb-0341-4db8-9bb1-16caecfedc51",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not an error, but Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\n",
      "are not enabled or a bias term (like in Qwen) is used.\n",
      "Unsloth 2026.1.4 patched 16 layers with 16 QKV layers, 16 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapters added\n"
     ]
    }
   ],
   "source": [
    "# LoRA time\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    lora_alpha = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    ")\n",
    "print(\"LoRA adapters added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "547b57b6-89c8-4efb-9110-c26bbeb15050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model): #simple data, like trainable parameters, total params, %training\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    return trainable, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9989cb3-a1ca-41d2-992b-4dd63e72a32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 3,407,872\n",
      "Total: 777,848,832\n",
      "Percentage: 0.44%\n"
     ]
    }
   ],
   "source": [
    "trainable, total = count_parameters(model)\n",
    "print(f\"Trainable: {trainable:,}\")\n",
    "print(f\"Total: {total:,}\")\n",
    "print(f\"Percentage: {100 * trainable / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0916c2ca-82da-49a7-a53e-696607ddbbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d073bb1-96ed-4db1-877a-d8ee3468c2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace contents:\n",
      "['.cache', '.gitignore', '.ipynb_checkpoints', '.Trash-0', 'basic_search_examples.json', 'data', 'discovery.ipynb', 'error_recovery_examples.json', 'keyword_retry_examples.json', 'llmquery_examples.json', 'multistep_examples.json', 'outputs', 'supervised-fine-tuning.ipynb', 'unsloth_compiled_cache']\n"
     ]
    }
   ],
   "source": [
    "#figure out the file structure\n",
    "import os\n",
    "print(\"Workspace contents:\")                                                                                                            \n",
    "print(os.listdir(\"/workspace\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "035770e7-1c7b-44a2-a0e5-fa96de1e260d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 56 training examples\n",
      "First example preview:\n",
      " [user]: You are a SEARCH assistant with a Python REPL. You search documents - nothing el...\n",
      " [assistant]: ```python\n",
      "idx = context.find('speed limit')\n",
      "print(f\"Found at: {idx}\")\n",
      "```\n",
      " [user]: Output:\n",
      "Found at: 2847\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/workspace/data/rlm_training_full.json\"\n",
    "with open(data_path, \"r\") as f:\n",
    "    training_data = json.load(f)\n",
    "dataset = Dataset.from_list(training_data)\n",
    "print(f\"loaded {len(dataset)} training examples\")\n",
    "print(f\"First example preview:\")\n",
    "for msg in dataset[0]['messages'][:3]:\n",
    "    role = msg['role']\n",
    "    content = msg['content'][:80] + \"...\" if len(msg['content']) > 80 else msg['content']\n",
    "    print(f\" [{role}]: {content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0fdede49-15c1-44b8-8cb2-f275780e8be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497b0e94972d43f8b7bb663b864b6232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted dataset columns: ['text']\n",
      "\n",
      "First example preview (first 300 chars):\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 01 Feb 2026\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "You are a SEARCH assistant with a Python REPL. You search documents - nothing else.\n",
      "\n",
      "OUTPUT FORMAT: Your response must START\n"
     ]
    }
   ],
   "source": [
    "  # Step 1: Pre-format all examples                                                                                                  \n",
    "def format_chat(example):\n",
    "  return tokenizer.apply_chat_template(                                                                                        \n",
    "      example[\"messages\"],\n",
    "      tokenize=False,\n",
    "  )\n",
    "\n",
    "# Apply formatting to create \"text\" column\n",
    "formatted_dataset = dataset.map(\n",
    "  lambda x: {\"text\": format_chat(x)},\n",
    "  remove_columns=[\"messages\"]  # Remove old column, keep \"text\"\n",
    ")\n",
    "\n",
    "print(\"Formatted dataset columns:\", formatted_dataset.column_names)\n",
    "print(\"\\nFirst example preview (first 300 chars):\")\n",
    "print(formatted_dataset[0][\"text\"][:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8989ffac-5327-47ff-817e-52a5b45c919c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaec6c7a30c94979871f8f0fc656bf35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=28):   0%|          | 0/56 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(                                                                                                                  \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,                                                                                                         \n",
    "    train_dataset=formatted_dataset,\n",
    "    max_seq_length=2048,\n",
    "    dataset_text_field = \"text\",\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./rlm_lora_output\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=1,\n",
    "        save_strategy=\"epoch\",\n",
    "        bf16=True,\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c1860e1-67ec-47d5-a892-e8754ce6533a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 56 | Num Epochs = 3 | Total steps = 21\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 3,407,872 of 1,239,222,272 (0.28% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 00:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.209100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.162600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.069600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.925800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.807400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.657000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.551700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.549700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.385800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.304700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.229000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.351500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.180800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.272200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.134900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.120700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.015700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.973500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.151600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.883800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.055600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING URL not available in offline run\n",
      "wandb: WARNING URL not available in offline run\n",
      "wandb: WARNING URL not available in offline run\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br><code>wandb sync /workspace/wandb/offline-run-20260201_022336-q0jlpopg<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/offline-run-20260201_022336-q0jlpopg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f1018945-6521-4a31-a77f-94ec60bea89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Response: \n",
      "```python\n",
      "from contextlib import contextmanager\n",
      "\n",
      "@contextmanager\n",
      "def search_context():\n",
      "    \"\"\"Search context variable for search results\"\"\"\n",
      "    try:\n",
      "        yield\n",
      "    finally:\n",
      "        context = yield\n",
      "        print(context)\n",
      "\n",
      "# Search context\n",
      "with search_context() as context:\n",
      "    print(context)  # \"Acme Corp was established in 1987 by John Smith in Seattle. The company grew rapidly...\"\n",
      "\n",
      "# Search for specific text\n",
      "print(context.find(\"The company grew rapidly...\"))  # Output: \"1987\"\n",
      "\n",
      "# Search for a range of text\n",
      "print(context[start:end])  # Output: \"The company grew rapidly... 1987-\"\n",
      "\n",
      "# Search for a specific text in a range\n",
      "print(context[start:end])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "test_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"\"\"You are a SEARCH assistant with a Python REPL. You search documents - nothing else.\n",
    "\n",
    "  OUTPUT FORMAT: Your response must START with a Python code block using ```python\n",
    "\n",
    "  AVAILABLE:\n",
    "  - context: the document text (string) - already loaded, do NOT redefine it\n",
    "  - print(): to output results\n",
    "\n",
    "  RULES:\n",
    "  - Search FIRST, answer NEVER until you have evidence\n",
    "  - Use context.find() or context[start:end] to explore\n",
    "  - Only give FINAL(answer) when you have proof\n",
    "\n",
    "  ---\n",
    "\n",
    "  The context variable already contains: \"Acme Corp was established in 1987 by John Smith in Seattle. The company grew rapidly...\" \n",
    "\n",
    "  Question: What year was the company founded?\"\"\"}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    test_messages,\n",
    "    return_tensors=\"pt\",\n",
    "    add_generation_prompt = True\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens = 150,\n",
    "    temperature = .3, # the lower, the more deterministic\n",
    "    do_sample = True,\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens = True)\n",
    "\n",
    "print(\"Done. Response: \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c91760f-fe98-4cb2-8b00-cac1c405745d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to ./rlm/lora_adapter\n"
     ]
    }
   ],
   "source": [
    "# save progress\n",
    "model.save_pretrained(\"./rlm_lora_adapter\")\n",
    "tokenizer.save_pretrained(\"./rlm_lora_adapter\")\n",
    "\n",
    "print(\"saved to ./rlm/lora_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9991b1-7ed1-4957-9fd2-95fc47663b53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
