{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79e214ee-e91a-466d-a880-28bb8635f4bd",
   "metadata": {},
   "source": [
    "# This script seeks to create a transformer by taking it from the Tiny Shakespeare's Dataset to generate infinite (but completely random) Shakespeare-like text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a009c9-6674-43a0-8113-07bd7000458f",
   "metadata": {},
   "source": [
    "## First, lets import the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d92c5c75-e062-468d-b6b8-a96eafd59702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-02-07 23:21:34--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.1’\n",
      "\n",
      "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.08s   \n",
      "\n",
      "2026-02-07 23:21:34 (13.2 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de243eed-839c-47ad-a351-cf8642a5d48f",
   "metadata": {},
   "source": [
    "### Now lets read it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00799539-60fa-46b4-82ac-9a790feef598",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read() #saves the entire file to one large string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb45c3d1-d9ad-43c6-83b3-01b29265515c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of dataset in characters: {len(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0735a1-ff36-48a6-aadf-0b8a8a5fdc34",
   "metadata": {},
   "source": [
    "Then lets fetch the unique characters in this text to fetch our vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a11fec45-202e-4246-b228-ecd5e76e69be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_amount = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1799385-061e-47ff-bb9c-ebf92318afd3",
   "metadata": {},
   "source": [
    "Now lets try to tokenize the input text from raw text to some vector of notebooks from the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59c82046-0723-4fe9-baaf-4d5e5eb5f4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 6, 1, 42, 39, 60, 47, 42, 1, 47, 57, 1, 39, 61, 43, 57, 53, 51, 43]\n",
      "hello, david is awesome\n"
     ]
    }
   ],
   "source": [
    "str_int = { ch : i for i, ch in enumerate(chars) } # for encoding\n",
    "int_str = { i : ch for i, ch in enumerate(chars) } # for decoding\n",
    "encode = lambda s: [str_int[ch] for ch in s]\n",
    "decode = lambda i: ''.join(int_str[d] for d in i)\n",
    "\n",
    "# let's test it out\n",
    "\n",
    "print(encode(\"hello, david is awesome\"))\n",
    "print(decode(encode(\"hello, david is awesome\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f7e1a9-b9f6-45b6-b2df-06030b7d803e",
   "metadata": {},
   "source": [
    "To encode the entire test dataset we need to import PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19ed1abe-ab57-4567-bca2-c222096f41c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3278d6b6-622c-4404-aec4-cec0034603c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype = torch.long) # Take all of the text from tiny shakespeare and encode it, then wrap to a tensor.\n",
    "\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # Only the first 1,000 characters tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249af8be-567e-4a72-8577-c8075a92d579",
   "metadata": {},
   "source": [
    "Let's now seperate our data into train and validation sets. Specifically a 90-10 split.\n",
    "This means that we will keep 90% of the data and withhold the last 10% to validate so \n",
    "we can see how much it overfits as we don't want this LLM memorizing the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "534f8549-0798-487f-9fd4-970c6898cf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003854\n"
     ]
    }
   ],
   "source": [
    "n = int(.9 * len(data))\n",
    "print(n)\n",
    "train = data[:n]\n",
    "val = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc07ab76-c747-44ba-8296-174409829b67",
   "metadata": {},
   "source": [
    "### Now its time to actually implement a transformer to train and learn these patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f44fd9-6bdc-48c3-b4ea-022b93fd8449",
   "metadata": {},
   "source": [
    "#### It's important to note that training transformers isn't just slapping the entire dataset in because when the data is large that can be very computationally demanding. Instead, we only work with \"chunks\" of the data instead, and sample random chunks out of the set to train chunks of length k at max, which typically is referred to as \"block_size\", or \"context_length\". In our example block_size will be 8. But in modern days block_size has advanced from sizes of 512-2048 in GPT-3 to 8k-128K+ in models like Opus 4.6 due to improvements in attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e401783-bb4a-446f-bc12-a427cae2b166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train[:block_size + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1248f64f-e864-42b4-83e8-2d8e85c45a38",
   "metadata": {},
   "source": [
    "We pack 9 indexes in this example because transformers update as they traverse the data. Therefore 9 indexes results in 8 interactions. \n",
    "\n",
    "e.g: \\\n",
    "We see 18, and contextualize that 47 likely comes next. \\\n",
    "We see 18, 47, then contextualize that 56 likely comes next, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c04645aa-c540-4c08-8613-1b0e4de96c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the input is tensor([18]) the target is 47.\n",
      "When the input is tensor([18, 47]) the target is 56.\n",
      "When the input is tensor([18, 47, 56]) the target is 57.\n",
      "When the input is tensor([18, 47, 56, 57]) the target is 58.\n",
      "When the input is tensor([18, 47, 56, 57, 58]) the target is 1.\n",
      "When the input is tensor([18, 47, 56, 57, 58,  1]) the target is 15.\n",
      "When the input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47.\n",
      "When the input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58.\n"
     ]
    }
   ],
   "source": [
    "x = train[:block_size]\n",
    "y = train[1:block_size + 1]\n",
    "\n",
    "for i in range(block_size):\n",
    "    context = x[:i + 1]\n",
    "    target = y[i]\n",
    "    print(f\"When the input is {context} the target is {target}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae2d6df-489e-4137-805e-be6bd1786da9",
   "metadata": {},
   "source": [
    "Note: the for loop is for visualization, but blocks x and y are what are actually fed into PyTorch. \\\n",
    "Imagine an input x, a block containing [0, 1, 2, 3, 4, 5, 6, 7]  \\\n",
    "And a target y, another block containing [1, 2, 3, 4, 5, 6, 7, 8] \\\n",
    "The model predicts what the next token would be at each position (without looking into the future), then compares the prediction against y to compute loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50717459-8064-48b7-891c-562086ca61cf",
   "metadata": {},
   "source": [
    "#### The cool part about GPU's is that many cores can work on completely seperate things without ever having to communicate with each other, so now let's generalize the above example to a wider scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bf86ab6-5639-4cd6-88a6-f9e539edde61",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337) # sets the random seed so that we can get the same result for example purposes.\n",
    "batch_size = 4 # 4 concurrent processes that forward-pass and backwards-pass\n",
    "block_size = 8 # max context length of 8 in predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train if split == 'train' else val # we are shadowing the global data. this is just some random local \"data variable\"\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # Here you can see that duplicate data IS possible, but that is the point. We want completely random data\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    return x, y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edb9d35-67a9-4ca3-95b9-9aae40b88ee2",
   "metadata": {},
   "source": [
    "Lets try it out: you should see that the targets are just offset by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "133b939b-15a9-4a33-b3b8-6495725f0bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print(f'inputs:\\n{xb.shape}\\n{xb}\\ntargets:\\n{yb.shape}\\n{yb}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb6c64e-1316-4f33-95f4-3ae641ece285",
   "metadata": {},
   "source": [
    "#### This gives us 4 completely independent examples (x) that will be fed into the transformer which will then be compared to our targets (y)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4684741c-b41b-4aef-a6fc-84f31edc15f1",
   "metadata": {},
   "source": [
    "### Now it's time to feed this to a neural network. For simplicity we will use the bigram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4be1a8c1-9f44-408f-908c-643ed0457269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn # Neural Network, stateful\n",
    "from torch.nn import functional as F # returns a value for a function (like softmax, Relu, etc) and doesn't need to persist. Stateless\n",
    "\n",
    "torch.manual_seed(1337) #same seed as before\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__() # calls the __init__ of the parent class to initialize and store weights for future use.\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # creates a 65 x 65 (in this case) matrix of random normal distributed values. Over time it gets trained to higher accuracy\n",
    "\n",
    "    def forward(self, idx, targets = None): # None by default so generate() can use it.\n",
    "\n",
    "        logits = self.token_embedding_table(idx) # replaces every single token with a 65 character row for the corresponding row. e.g. 43 becomes the entire 65 character row 43.\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None # doesnt return anything since its not comparing to anything\n",
    "        else:\n",
    "            # cross entropy doesn't want a B, T, C shape, it wants B, C, T. \n",
    "            B, T, C = logits.shape\n",
    "            logits  = logits.view(B*T, C) # PyTorch doenst expect a 3D matrix, so we convert a 4 x 8 matrix with a 65 row result into 32 rows each containing a 65 row tensor.\n",
    "            targets = targets.view(B*T) # Same idea here\n",
    "        \n",
    "            loss = F.cross_entropy(logits, targets) \n",
    "        \"\"\"\n",
    "        Cross Entropy basically compares predictions from Logits to expected answers. Basically from logits it softmaxes it and converts each value\n",
    "        into probabilities all summing to 1. Then it takes the -ln() of each value. Then PyTorch takes the actual answer it shouldve been and \n",
    "        compares it to the negative log of the correct token. After calculating the loss value at each token it averages it and that is what\n",
    "        is spit out by loss.\n",
    "        \"\"\"\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            logits, loss = self(idx) # grab predictions\n",
    "            logits = logits[:, -1, :] # becomes (B, C). this is a bigram model so we only care about the most recent output to generate a new token.\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            idx_next  = torch.multinomial(probs, num_samples = 1) # (B, 1). Selects a random value based on percentages. even something with a .01% of being chosen COULD be chosen\n",
    "            idx = torch.cat((idx, idx_next), dim = 1) #(B, T + 1) appends the value to idx.\n",
    "            \n",
    "        return idx\n",
    "\n",
    "    \"\"\" \n",
    "    This generate function is pretty dumb because we are still feeding it the ENTIRE context, but only reading the last token to make a prediction on. \n",
    "    That's because this is a bigram model that only cares about the last token but in the future we will have Models that actually see the whole\n",
    "    context (or at least as much as it can) so we are keeping the generate function wide so that future code changes will stay the same for \n",
    "    consistency.\n",
    "    \"\"\"\n",
    "\n",
    "        \n",
    "m = BigramLanguageModel(vocab_amount) # creates a BLM of a vocab_amount x vocab_amount matrix.\n",
    "\n",
    "logits, loss = m(xb, yb) # indirectly calls __call__ in BigramLanguageModel which calls forward.\n",
    "print(logits.shape)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1cf857-a5f5-4565-9cda-4f745b3f0896",
   "metadata": {},
   "source": [
    "In our first run, we got a loss of ~4.8786. Very high but to be expected as this is a completely new embedding table. Theoretically since this is pretty random the loss should've been around -ln(1/65) or ln(65) but that assumes the neural network was like \"hey i have 0 clue, lets use everything equally). But these values do NOT have equal probabilities so some values inevitably got used more and less than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe16344b-0b47-4854-9479-c663ef5e803b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SBLBcHAUhk&PHdhcOb\n",
      "nhJ?FJU?pRiOLQeUN!BxjPLiq-GJdUV'hsnla!murI!IM?SPNPq?VgC'R\n",
      "pD3cLv-bxn-tL!upg\n",
      "SZ!Uv\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype = torch.long), max_new_tokens = 100)[0].tolist()))\n",
    "# This code basically predicts what goes on after a single token 0, which is a new line in our example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296d567a-ca0e-444f-b184-41edbe4976a9",
   "metadata": {},
   "source": [
    "#### So the first run ever, this is the output:\n",
    "\n",
    "SBLBcHAUhk&PHdhcOb \\\n",
    "nhJ?FJU?pRiOLQeUN!BxjPLiq-GJdUV'hsnla!murI!IM?SPNPq?VgC'R \\ \n",
    "pD3cLv-bxn-tL!upg \\\n",
    "SZ!Uv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84bed7b-db3e-45e2-935d-46e454e1924c",
   "metadata": {},
   "source": [
    "#### This is pretty garbage, but luckily that's to be expected since this model is completely random. It's now time to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a7c8b69-ba54-4fbf-be7f-6b61866b5844",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8d5f3a-fc3f-4e9f-859d-ef9b6fe726c3",
   "metadata": {},
   "source": [
    "this is a PyTorch optimizer. \n",
    "an optimizer actually updates those weights stored in __init__.\n",
    "AdamW is a smarter way of optimizes that asks 2 main questions:\n",
    " - How big were recent gradient pushes? (if it's been moving in tiny pushes, give it a larger push this time)\n",
    " - What direction have gradients been going? (if it keeps moving in one direction, carry the momentum)\n",
    "\n",
    "lr, or learning rate of 1e-3 means that the scaling factor. \n",
    "\n",
    "e.g.\n",
    "\n",
    "new_weight = old_weight - lr x other gradient stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7ae697e-b17d-4d49-b808-df87ce104ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5693893432617188\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(10000):\n",
    "\n",
    "    xb, yb = get_batch('train') # First creates the batch\n",
    "\n",
    "    logits, loss = m(xb, yb) # Gets the logits and loss from the batches\n",
    "    optimizer.zero_grad(set_to_none = True) # Resets each gradient for fresh usage from previous .step, .backwards.\n",
    "    loss.backward() # Computes new gradients in self.token_embedding_table.grad. Identical size but instead it shows what should change in each one.\n",
    "    optimizer.step() # Adjusts actual weights based on .grad.\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fef1c23-2c9f-42c2-b31f-d50304c888e4",
   "metadata": {},
   "source": [
    "#### After running the above a couple times and seeing the loss go down, lets check out the new guess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d68cb8db-eebb-4906-ab59-60a4db7ac416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "T athe ce! oungleasitasend devibathe pe tulaitowitt,\n",
      "ICIn witere at?\n",
      "obl,\n",
      "Thalul y ou s s t pordel be\n",
      "Whay st ILI s Ithelonew,\n",
      "LO Ed athothake, t u were Youeje the-thery blouther bowat towithaianattt kntr I se y ce sech is arsenased tthentsman lemout:\n",
      "I\n",
      "We;\n",
      "\n",
      "\n",
      "NGle, LAndr!\n",
      "Fimau f,\n",
      "QUTirourthithak, malit I myon Ispoupe mom;\n",
      "H:\n",
      "hades cks thaker de benlleall in\n",
      "OUETCHUETHare t VABUSAmie irrowaks\n",
      "\n",
      "Had\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype = torch.long), max_new_tokens = 400)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11794ecb-9064-469f-8932-a176f7dfbd99",
   "metadata": {},
   "source": [
    "#### New output:\n",
    "\n",
    "\n",
    "T athe ce! oungleasitasend devibathe pe tulaitowitt, \\\n",
    "ICIn witere at? \\\n",
    "obl, \\\n",
    "Thalul y ou s s t pordel be \\\n",
    "Whay st ILI s Ithelonew, \\\n",
    "LO Ed athothake, t u were Youeje the-thery blouther bowat towithaianattt kntr I se y ce sech is arsenased tthentsman lemout: \\\n",
    "I \\\n",
    "We; \\\n",
    "\n",
    "\n",
    "NGle, LAndr! \\\n",
    "Fimau f, \\\n",
    "QUTirourthithak, malit I myon Ispoupe mom; \\\n",
    "H: \\\n",
    "hades cks thaker de benlleall in \\\n",
    "OUETCHUETHare t VABUSAmie irrowaks \\\n",
    "\n",
    "Had"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cca155e-f7fd-4889-a0a2-57083435839d",
   "metadata": {},
   "source": [
    "#### Still pretty crap, but is a huge improvement to before. It knows to occasionally add a space and not really use symbols or numbers that frequently and even made a real word at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddeadf3-e714-4700-9cb2-c8bbcc3375b8",
   "metadata": {},
   "source": [
    "The issue stopping this from creating coherent Shakespeare is that a Bigram model doesn't coordinate tokens __with__ each other, which is what makes words function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
